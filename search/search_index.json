{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Summary","text":"<p>These are the teaching materials for the course Aprendizaje Profundo (deep learning), coordinated by Antonio Pertusa and also taught by professors Juan Antonio P\u00e9rez, Andr\u00e9s Fuster, Jorge Azor\u00edn, and Jorge Calvo.</p> <p>For information regarding the course assessment, please refer to the Moodle contents at UACloud and the course official info page. </p>"},{"location":"#methodology-and-evaluation","title":"Methodology and evaluation","text":"<ul> <li>The theory contents should be revised by the student before attending the practical sessions. A test will be answered to ensure that these contents are read and understood by the student. </li> <li>Practical assignments (jupyter notebooks) are to be completed as indicated in each assignment description. Each of the course blocks will have one or more practical assignments. </li> </ul> <p>The theory tests represent 30% of the final grade.  The assignments represent 70% of the final grade.</p>"},{"location":"#schedule","title":"Schedule","text":"<p>The course has the following blocks:</p> <ul> <li>Feb. 5: From Shallow to Deep Neural Networks (Andr\u00e9s Fuster)</li> <li>Feb. 5 &amp; Feb. 12: Convolutional Neural Networks (Jorge Azor\u00edn)</li> <li>Feb 12: Few and Zero Shot Learning (Antonio Pertusa)</li> <li>Feb. 19: Deep Reinforcement Learning (Jorge Calvo)</li> <li>Feb. 26: Recurrent architectures (Juan Antonio P\u00e9rez)</li> <li>Feb. 26: Fine tuning and knowledge distillation (Juan Antonio P\u00e9rez)</li> <li>Mar. 5: Self-Supervised Learning (Antonio Pertusa)</li> </ul>"},{"location":"#more-information","title":"More information","text":"<p>The source code of these pages, written in markdown for MkDocs, is available on GitHub.</p> <p>You can obtain a local copy of these pages (e.g., for offline access) by executing:</p> <pre><code>wget --mirror --no-parent --convert-links --page-requisites https://pertusa.github.io/ap\n</code></pre> <p>Please note that the content may change throughout the course.</p>"},{"location":"cnn/","title":"Convolutional Neural Networks (CNN)","text":"<p>Convolutional Neural Networks (CNNs) are specifically tailored for computer vision tasks (classification, detection, segmentation, synthesis, etc.) (See Chapter 10.1 [FDL2023]). In 1989, LeCun proposed LeNet, a CNN for recognizing handwritten digits in images that was trained by backpropagation. It was widely recognised as the first CNN model achievieng outstanding results matching the performance of support vector machines, then a dominant approach in supervised learning. It laid the foundation for modern CNN architectures and demonstrated the power of convolutional layers and their ability to learn spatial hierarchies of features in an image, a principle that remains central in modern CNNs used for more complex tasks in computer vision.  </p> <p>However, CNNs got popular in 2012 when outperformed other models at ImageNet Challenge Competition in object classification/detection (here you can see a visualization hierarchy of 1000 classes from ImageNet). Specifically, the first CNN to achieve a breakthrough in the ImageNet Challenge was AlexNet (designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton). It significantly outperformed the other competitors in the 2012 challenge, reducing the error rate by a large margin compared to traditional image classification methods (see here the results. </p> <p>Today, the results of the DNN (including CNNs and others as Visual Transformers) in ImageNet Challege Competion are achieving amazing results (more than 95% Top-1 accuracy).  </p>"},{"location":"cnn/#objectives","title":"Objectives","text":"<ul> <li>O1. Know and implement the fundamental components that conform a CNN.</li> <li>O2. Learn about modern convolutional networks that have set milestones in design aspects and how to train them</li> </ul>"},{"location":"cnn/#1-first-session-of-this-block-5th-february-2025","title":"1.  First session of this block (5th February 2025)","text":""},{"location":"cnn/#contents-to-prepare-before-online","title":"Contents to prepare before (online)","text":"<p>The contents of this first session are related to the objetive 1, being the following:</p>"},{"location":"cnn/#11-introduction","title":"1.1 Introduction","text":"<pre><code>[This part can take about 1 hour \ud83d\udd52\ufe0f of personal working.]\n</code></pre> <ul> <li> <p>Convolutions </p> <ul> <li>Convolution (or cross-correlation operation): [DDL23, Section 7.1.3]</li> <li>Examples of kernels: image kernels</li> </ul> </li> <li> <p>Why CNN? [UDL2023, Section 10 at beginning],[DDL23, Section 7.1], [DDL23, Section 7.1.2].</p> <ul> <li>Reduction of learning parameters </li> <li>Invariance: [UDL2023, Section 10.1], [DDL23, Section 7.1.2.1]</li> <li>Locality principle: [DDL23, Section 7.1.2.2]</li> </ul> <p>NOTE: You don't need to go deeper into the mathematical formulation</p> </li> <li> <p>Architecture of a CNN: a typical CNN has 4 layers: Input layer, Convolution layer, Pooling layer and Fully connected layer. </p> </li> </ul>"},{"location":"cnn/#12-convolutional-layer","title":"1.2 Convolutional layer","text":"<pre><code>[This part can take about 2 hours \ud83d\udd52\ufe0f of personal working.]\n</code></pre> <p>A convolutional layer is the fundamental building block of a CNN. It is able to detect features such as edges, textures, or more complex patterns in higher layers from the input images, extracting characteristics from the previous layers (input layer, previous convolutions,\u2026). A very interesting description of these layers could be found in this web. It includes the following concepts:</p> <ul> <li>Padding: Image (n,m), filter (f,f), padding p -&gt; Out (n+2p-f+1, m+2p-f+1)</li> <li>Strides: Image (n,m), filter (f,f), padding p, stride s -&gt; floor((n+2p-f)/s+1), floor((m+2p-f)/s+1)</li> <li>Convolutions over volumes</li> <li>Multiple filters</li> </ul> <p>After the convolution operation, the non-linearity is introduced by an activation function (Sigmoid, ReLU, etc.). It allows to learn more complex patterns.</p> <p>Some of the contents are extracted from this paper. A summarized version of the concepts can be found in the section 10.2 (except 10.2.6 and 10.2.8) from [FDL2023]. Finally, to go in depth with these concepts, I recomnend you to read the section Convolutions for images, Padding and Stride and Multiple Input and Multiple Output from the Chapter 7 of the [DDL2023] book.</p> <p>Notes: </p> <ul> <li>The result of a convolution filter with size ) to an image of  size with a padding  is:      </li> <li>The result of a convolution filter with size  to an image of  size with a padding  and a stride of  is:  </li> <li>Let  the number of channels of the previous layer  of a convolutonal layer,  the filter height and widht and  the number of filters in the layer. The number of parameters of the convulational layer is:  </li> </ul>"},{"location":"cnn/#exercise","title":"Exercise","text":"<ol> <li>Calculate the size of the filters of the different layers (a,b,c), (d,e,f) and (g,h,i) of the following image:</li> </ol>"},{"location":"cnn/#13-pooling-layer","title":"1.3 Pooling layer","text":"<pre><code>[This part can take about 1 hour \ud83d\udd52\ufe0f of personal working.]\n</code></pre> <p>The main function of the pooling layer is to reduce the spatial dimensions (i.e., width and height) of the input volume for the next convolutional layer. This reduction is achieved without affecting the number of filters in the layer. The pooling operation provides several benefits:</p> <ol> <li>Reduction of computation: By reducing the dimensions of the feature maps, pooling layers decrease the number of parameters and computations in the network, leading to improved computational efficiency.</li> <li>Reduction of Overfitting: Smaller input sizes mean fewer parameters, which can help reduce the overfitting in the network.</li> <li>Invariance to Transformations: Pooling helps the network to become invariant to small transformations, distortions, and translations in the input image. This means that the network can recognize the object even if it's slightly modified in different input images.</li> </ol> <p>There are several types of pooling, but the most common are:</p> <ul> <li>Max Pooling</li> <li>Average Pooling</li> </ul> <p>A more detailed explanation could be found in the Pooling section from the Chapter 7 of the [DDL2023] book.</p> <p>Notes: </p> <ul> <li>There is not parameters to learn!</li> </ul>"},{"location":"cnn/#contents-for-the-presential-class","title":"Contents for the presential class","text":"<p>In the laboratory class (2 hours \ud83d\udd52\ufe0f duration), we will see how certain components of a convolutional neural network are implemented .</p> <p>The aim is for the notebooks to be studied and modified. A later class will present a more advanced practice that will involve modifying and implementing CNN code.</p>"},{"location":"cnn/#2-second-session-of-this-block-12th-february-2025","title":"2.  Second session of this block (12th February 2025)","text":""},{"location":"cnn/#contents-to-prepare-before-online_1","title":"Contents to prepare before (online)","text":"<p>The contents of this first session are related to the objetive 2, being the following:</p>"},{"location":"cnn/#21-introduction","title":"2.1 Introduction","text":"<pre><code>[This part can take about 0,5 hours \ud83d\udd52\ufe0f of personal working.]\n</code></pre> <p>A typical CNN has several convolution plus pooling layers, each responsible for feature extraction at different levels of abstraction: filters in first layer detect horizontal, vertical, and diagonal edge; filters in the next layer detect shapes; filters in the following layers detect collection of shapes, etc.</p> <p>A good starting point to understand the architecture of a simple CNN is to study the LeNet model designed in the 90s. LeNet, one of the earliest convolutional neural networks, was designed by Yann LeCun et al. for handwritten and machine-printed character recognition. It laid the groundwork for many of the CNN architectures that followed. LeNet is relatively small by today's standards, with approximately 60K parameters. This makes it computationally efficient and easy to understand. LeNet's architecture reduces width and height dimensions through its layers, while increasing the depth (number of filters). This reduction is achieved through the use of convolutional layers with strides and pooling layers, which also help in achieving spatial invariance to input distortions and shifts. The composition of layers is:</p> <ul> <li><code>Input Layer</code>: The original LeNet was designed for 32x32 pixel input images.</li> <li><code>Convolutional Layer</code>: The first convolutional layer uses a set of learnable filters. Each filter produces one feature map, capturing basic features like edges or corners. </li> <li><code>Pooling Layer</code>: Follows the first convolutional layer, reducing the spatial size (width and height) of the input volume for the next convolutional layer, reducing the number of parameters and computation in the network, and hence also controlling overfitting.</li> <li><code>Convolutional Layer</code>: A second convolutional layer that further processes the features from the previous pooling layer, detecting higher-level features.</li> <li><code>Pooling Layer</code>: this layer further reduces the dimensionality of the feature maps.</li> <li><code>Fully Connected Layer</code>: The flattened output from the previous layer is fed into a fully connected layer that begins the high-level reasoning process in the network.</li> <li><code>Fully Connected Layer</code>: An additional fully connected layer to continue the pattern analysis from the previous layer, leading to the final classification.</li> <li><code>Output Layer</code>: The final layer uses a softmax to output the probabilities for each class.</li> </ul> <p>Notes * In some versions of LeNet, the sigmoid function and the hyperbolic tangent (tanh) function were used as the activation function in the convolutional and fully connected layers. </p>"},{"location":"cnn/#exercise_1","title":"Exercise","text":"<ol> <li>Calculate the number of the learning parameters of LeNet architecture</li> </ol>"},{"location":"cnn/#22-classic-networks","title":"2.2. Classic networks","text":"<pre><code>[This part can take about 1 hour \ud83d\udd52\ufe0f of personal working.]\n</code></pre> <ul> <li> <p>Alexnet: [DDL23, Section 8.1]</p> <ul> <li>AlexNet represents a significant milestone in the development of convolutional neural networks and played a pivotal role in demonstrating the power of deep learning for image recognition tasks.</li> <li>It was significantly larger and deeper than its predecessors like LeNet (with about 60M parameters).  This increase in scale allowed AlexNet to capture more complex and abstract features from images, contributing to its superior performance.</li> <li>It was one of the first CNNs to successfully use ReLU activation functions instead of the sigmoid or tanh functions that were common at the time. ReLUs help to alleviate the vanishing gradient problem, allowing deeper networks to be trained more effectively.</li> <li>It introduced overlapping pooling, where the pooling windows overlap with each other, as opposed to the non-overlapping pooling used in earlier architectures like LeNet. This was found to reduce overfitting and improve the network's performance.</li> <li>It introduced data augmentation techniques such as image translations, horizontal reflections, and alterations to the intensities of the RGB channels.</li> <li>Layers:<ul> <li><code>Input Layer</code>: The network accepts an input image size of 227x227 pixels with 3 color channels (RGB).</li> <li><code>First Convolutional Layer (Conv1)</code>: It uses 96 kernels of size 11x11 with a stride of 4 and applies ReLU activation. This large kernel size is chosen for the first convolutional layer to capture the low-level features from the larger input image.</li> <li><code>Max Pooling Layer</code>: kernel size of 3x3 and a stride of 2.</li> <li><code>Second Convolutional Layer (Conv2)</code>: It has 256 kernels of size 5x5, with padding applied to preserve the spatial dimensions. ReLU activation is used.</li> <li><code>Max Pooling Layer</code>: kernel size of 3x3 and a stride of 2.</li> <li><code>Third Convolutional Layer (Conv3)</code>: It has 384 kernels of size 3x3, with padding and ReLU activation.</li> <li><code>Fourth Convolutional Layer (Conv4)</code>: Similar to Conv3, it has 384 kernels of size 3x3 with padding and ReLU activation.</li> <li><code>Fifth Convolutional Layer (Conv5)</code>: It has 256 kernels of size 3x3, again with padding and ReLU activation.</li> <li><code>Max Pooling Layer</code>: kernel size of 3x3 and a stride of 2.</li> <li><code>Fully Connected Layer (FC6)</code>: This dense layer has 4096 neurons and includes ReLU activation and dropout with a dropout rate of 0.5 to prevent overfitting.</li> <li><code>Fully Connected Layer (FC7)</code>: Also consists of 4096 neurons with ReLU activation and dropout.</li> <li><code>Fully Connected Layer (FC8)</code>: / The final FC layer has 1000 neurons (corresponding to the 1000 classes in the ImageNet challenge) </li> <li><code>Output layer</code>: Softmax activation function to output the probability distribution over the classes.</li> </ul> </li> </ul> </li> <li> <p>VGG-16 [DDL23, Section 8.2]</p> <ul> <li>VGG-16, developed by the Visual Graphics Group (VGG) at Oxford, is known for its simplicity and depth. It was a runner-up in the 2014 ImageNet competition.</li> <li>The architecture's significant number of parameters (138M) makes it prone to overfitting, which is mitigated by using dropout and data augmentation techniques.</li> <li>The model is characterized by its use of 3x3 convolutional layers stacked on top of each other in increasing depth.</li> <li>It has an uniform architecture consistent of using 3x3 convolutional filters and 2x2 max-pooling layers throughout the network</li> <li>It duplicate filters, starting at 64, doubles after each max-pooling layer, following the sequence 64, 128, 256, 512, 512</li> <li>Layers:<ul> <li><code>Input Layer</code>: size 224x224 pixels with 3 channels (RGB)</li> <li><code>Conv1</code>: 2x [Conv3x3-64] + MaxPool (2x2, stride 2)</li> <li><code>Conv2</code>: 2x [Conv3x3-128] + MaxPool (2x2, stride 2)</li> <li><code>Conv3</code>: 3x [Conv3x3-256] + MaxPool (2x2, stride 2)</li> <li><code>Conv4</code>: 3x [Conv3x3-512] + MaxPool (2x2, stride 2)</li> <li><code>Conv5</code>: 3x [Conv3x3-512] + MaxPool (2x2, stride 2)</li> <li><code>FC1</code>: 4096 neurons, ReLU activation</li> <li><code>FC2</code>: 4096 neurons, ReLU activation</li> <li><code>FC3</code>: 1000 neurons (for 1000 ImageNet classes), Softmax activation</li> </ul> </li> </ul> </li> </ul>"},{"location":"cnn/#22-residual-networks","title":"2.2. Residual networks","text":"<pre><code>[This part can take about 0,5 hours \ud83d\udd52\ufe0f of personal working.]\n</code></pre> <ul> <li>ResNet [FDL2023, Section 9.5], [TDS],[DDL2023, Section 8.6]<ul> <li>ResNet (Residual Network) was introduced by He et al. in 2015 and won the ImageNet competition by a significant margin.</li> <li>It addresses the vanishing gradient problem in deep neural networks through the use of residual blocks, enabling the training of networks that are much deeper than previous architectures. This is by using shortcut connections that allow gradients to flow through the network more effectively. his approach allows for the construction of very deep networks (ResNet variants come in depths of 50, 101, 152 layers, and more) without degradation in performance due to vanishing gradients.</li> <li>The core building block of ResNet that enables the network to learn identity functions, ensuring that deeper layers can at least perform as well as shallower ones.</li> <li>A residual block allows the input to a series of layers to be added to their output, facilitating the learning process by allowing the network to learn modifications to the identity mapping rather than the entire transformation from scratch. It is composed by:<ul> <li>Shortcut Connection that skips one or more layers.</li> <li>Two or three convolutional layers, each followed by batch normalization and a ReLU activation function.</li> <li>The output of the weighted layers is added to the shortcut connection's output. If the input and output dimensions are the same, the shortcut connection directly adds the input  to the output of the convolutional layers , resulting in . If the dimensions of  and  do not match, a linear projection  is applied to  through a convolutional operation to match the dimensions. The resulting output is .</li> </ul> </li> <li>Layers of ResNet-34:<ul> <li><code>Input Layer</code>: size 224x224 pixels with 3 channels (RGB) </li> <li><code>Convolutional Layer</code>: 7x7 convolutions, 64 filters, stride of 2, ReLU Activation</li> <li><code>Max Pooling</code>: 3x3, stride of 2</li> <li><code>Residual blocks</code>: The blocks in each stage have the same number of filters, but the number of filters increases as the network deepens.<ul> <li>3 Basic Residual Blocks: Each block has two 3x3 convolutional layers with 64 filters each. Shortcut connections add the input of the block to its output without any modification since the dimensions match.</li> <li>4 Basic Residual Blocks: Each block has two 3x3 convolutional layers with 128 filters each. The first block uses a stride of 2 for down-sampling and a 1x1 convolution in the shortcut connection to match the increased depth.</li> <li>6 Basic Residual Blocks: Each block has two 3x3 convolutional layers with 256 filters each. Similar to the previous stage 2, the first block uses a stride of 2 for down-sampling, and the shortcut connection includes a 1x1 convolution to match the depth.</li> <li>3 Basic Residual Blocks: Each block has two 3x3 convolutional layers with 512 filters each. Again, the first block in this stage applies a stride of 2 for down-sampling, and the shortcut connection includes a 1x1 convolution to match the depth.</li> </ul> </li> <li><code>Global Average Pooling</code>: Applied after the last convolutional block to reduce spatial dimensions to 1x1.</li> <li><code>Fully Connected Layer</code>: Ends with a fully connected layer with 1000 neurons (for the 1000 classes of the ImageNet dataset), followed by a softmax activation for classification.</li> </ul> </li> </ul> </li> </ul> <p>[From He et al. 2015]</p> <p>Note:</p> <ul> <li>1x1 convolution: 1X1 Conv are used to increase/reduce the number of channels while introducing non-linearity.</li> </ul>"},{"location":"cnn/#23-inception","title":"2.3. Inception","text":"<pre><code>[This part can take about 0,5 hours \ud83d\udd52\ufe0f of personal working.]\n</code></pre> <p>The Inception architecture [DDL 2023, Section 8.4], particularly known from the GoogLeNet (Inception v1) model introduced in the 2014 ImageNet competition, is notable for its novel approach to convolutional network design. It introduced the \"Inception module,\" a building block that allows the network to choose from different filter sizes and operations within the same layer. Some important notes:</p> <ul> <li>The Inception architecture revolutionizes the design of convolutional layers by incorporating multiple filter sizes within the same module, allowing the network to adapt to various spatial hierarchies of features in images.</li> <li>The Inception module:<ul> <li>1x1 Convolutions: To perform dimensionality reduction or increase, reducing the computational cost and the number of parameters in the network and to increase the network's ability to capture nonlinearities without a significant increase in computational complexity.</li> <li>Multiple Filter Sizes: Within each Inception module, convolutional operations with different filter sizes (e.g., 1x1, 3x3, and 5x5) are performed in parallel to capture information from various spatial extents. The outputs of these parallel paths are concatenated along the channel dimension, allowing the network to decide which filters to emphasize for each new input.</li> <li>Pooling: Inception modules also include a parallel pooling path, typically max pooling, followed by 1x1 convolutions to reduce dimensionality before concatenation.</li> <li>Dimensionality Reduction: Before applying larger convolutions (e.g., 3x3 and 5x5), 1x1 convolutions are used for dimensionality reduction, decreasing the computational burden.</li> </ul> </li> <li>The Inception modules' combination of parallel convolutional paths with different filter sizes and 1x1 convolutions for dimensionality management allows the network to be both wide (in terms of capturing a broad range of features) and deep (in terms of layers), while maintaining computational efficiency. This design philosophy has been extended and refined in subsequent versions of the Inception architecture, such as Inception v2, v3, and v4, each introducing further optimizations and improvements.</li> </ul> <p></p> <p> [From Szegedy et al. 2014]</p>"},{"location":"cnn/#24-computational-efficient-networks","title":"2.4. Computational efficient networks","text":"<pre><code>[This part can take about 1 hour \ud83d\udd52\ufe0f of personal working.]\n</code></pre> <ul> <li> <p>MobileNet [Medium]</p> <ul> <li>The key innovation is in their efficient architectural design, aimed at reducing computational cost while maintaining high performance, especially on mobile and embedded devices.</li> <li> <p>The core innovation is the use of depthwise separable convolutions.</p> <ul> <li>Instead of using standard convolutions, it employs depthwise separable convolutions, which use a standard convolution into a depthwise convolution and a 1x1 pointwise convolution. *The input is first processed by a depthwise convolution , applying a single filter per input channel. This is followed by a pointwise convolution  convolutions that combines the outputs of the depthwise convolution, adjusting the depth as necessary.</li> <li>The computational cost is significantly reduced compared to standard convolutions.</li> </ul> </li> <li> <p>Example: </p> </li> </ul> </li> </ul> <p>For the following convolution, the number of calculations is <code>filter parameters</code> x <code>filter positions</code> x <code>humber of filters</code>. This is (3 x 3 x 3) x (10 x 10) x 5 = 13.500 operations.</p> <p></p> <p>In case, we would use depthwise separable convolutions, as in the following figure:</p> <p></p> <p>The number of calculations will be reduced. First, the depthwise convolutions are applied for each channel obtaining that the number of calculations is again <code>filter parameters</code> x <code>filter positions</code> x <code>humber of filters</code>. This is (3 x 3) x (10 x 10) x 3 = 2.700 operations. Moreover, the pointwise (1x1) convolution, would be: (1 x 1 x 3) x (10 x 10) x 5 = 1.500 operations. Added to the previous calculations, we have a total of 4.200 operations, about a 30% of the previous operations.</p> <p></p> <ul> <li> <p>MobileNetV2:</p> <ul> <li>It introduces residual connections similar to those in ResNet, but within the framework of inverted residual blocks. These connections allow the input to bypass one or more layers, facilitating the flow of gradients during training and mitigating the vanishing gradient problem.</li> <li>Inverted Residual Blocks<ul> <li><code>Pointwise convolution</code>: Each block starts with a 1x1 convolution that expands the input's depth, increasing the representation capacity and allowing the network to learn more complex functions.</li> <li><code>Depthwise Convolution</code>: Follows the expansion layer, applying spatial filtering within each channel.</li> <li><code>Projection Layer</code>: A 1x1 convolution that projects the expanded feature map back to a lower dimension, reducing the size and computational cost of the feature map.</li> </ul> </li> <li>This expansion-projection strategy increases the network's expressiveness while keeping the computational cost low by expanding the feature space only temporarily within the block.</li> </ul> </li> <li> <p>EfficientNet [Medium]:  </p> <ul> <li>These architectures systematically scale up CNNs in a more structured manner to achieve better efficiency and accuracy. The key innovation of EfficientNet is the use of a compound scaling method that uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients.</li> <li>It is able to achieve state-of-the-art accuracy with significantly fewer parameters and FLOPs (floating-point operations per second) compared to previous architectures.</li> </ul> </li> </ul>"},{"location":"cnn/#25-image-augmentation","title":"2.5. Image augmentation","text":"<pre><code>[This part can take about 0,25 hours \ud83d\udd52\ufe0f of personal working.]\n</code></pre> <p>Finally, image augmentation is a technique used to enhance the diversity of a training dataset without actually collecting new images. This is achieved by applying a series of random transformations to the existing images in the dataset, such as rotations, translations, flips, scaling, shearing, and color variations. These transformations produce altered versions of the images, which help the model generalize better from the training data, making it more robust to variations it might encounter in real-world data.</p> <p>The benefits are:     * Enhanced Generalization: Augmentation increases the diversity of the training set, helping the model generalize better to unseen data.     * Reduced Overfitting: By providing varied examples, it prevents the model from memorizing specific images.     * Improved Robustnes: Models become more robust to variations in input data, such as different angles, lighting conditions, and occlusions.</p>"},{"location":"cnn/#contents-for-the-presential-class_1","title":"Contents for the presential class","text":"<p>In the laboratory class (2 hours \ud83d\udd52\ufe0f duration), we will implement a convolutional neural network .</p> <p>The aim is for the notebook to be modified.</p>"},{"location":"cnn/#biblography","title":"Biblography","text":""},{"location":"cnn/#textbooks","title":"Textbooks","text":"<ol> <li>[DDL2023] Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J. Dive into Deep Learning. Cambridge University Press (2023)</li> <li>[UDL2023] Simon J.D. Prince. Understandig Deep Learning. MIT Press (2023).</li> <li>[FDL2023] Bishop, C.M., Bishop, H. (2024). Convolutional Networks. In: Deep Learning. Springer, Cham. https://doi.org/10.1007/978-3-031-45468-4_10 (2023)</li> </ol>"},{"location":"cnn/#webpages","title":"Webpages","text":"<ul> <li>https://cs231n.github.io/convolutional-networks/: CNN fundamentals</li> <li>https://towardsdatascience.com/residual-networks-resnets-cb474c7c834a: ResNet</li> <li>https://medium.com/analytics-vidhya/talented-mr-1x1-comprehensive-look-at-1x1-convolution-in-deep-learning-f6b355825578:1x1 convolution</li> </ul>"},{"location":"cnn/#others","title":"Others","text":"<ul> <li>https://arxiv.org/pdf/1603.07285.pdf: Extra information for Convolutional parameters</li> </ul>"},{"location":"dnn/","title":"From Shallow to Deep Neural Networks","text":"<p>Objectives</p> <ul> <li>To understand the fundamentals of a basic multilayer perceptron neural network MLP/ Shallow neural network. </li> <li>To understand the concept of a \u201cbasic neuron\u201d and the concepts of \u201cactivation function\u201d, \u201closs function\u201d and by extension the \u201ccost function\u201d. </li> <li>To understand the concept of \u201cgradient descent\u201d through the use of function derivatives and the chain rule. </li> <li>To understand the idea of forward and backward propagation as a basis for the search for minima in the gradient descent process. </li> <li>To analyze the basis of deep neural networks with multiple inner layers.</li> <li>Understanding hyperparameters in a deep neural network and some techniques for improving error.</li> </ul> <p>Contents</p> <ul> <li>Supervised learning fundamentals (cross validation, overfitting, \u2026)</li> <li>Logistic Regression fundamentals</li> <li>Activation Function types</li> <li>Loss function - Cost function</li> <li>Gradient Descent</li> <li>Logistic Regression Derivatives (chain rule)</li> <li>Forward and Backward propagation </li> <li>MLP (Multilayer Perceptron)/ Shallow neural networks</li> <li>Deep L-layer Neural Networks</li> <li>Dropout, early stopping, parameter initialization</li> </ul> <p>Methodology</p> <ul> <li>Self-study of the proposed theoretical materials. These materials are structured using bibliographical references to explain the theoretical concepts.</li> <li>Moodle Tests: Moodle tests are proposed to evaluate the theoretical contents and to guarantee the learning to tackle the practical part. These moodle tests must be taken before the practical session.</li> <li>Workbooks: Several practical workbooks are proposed to improve the skills of design, implementation and configuration of neural network models.</li> </ul> <p>Recommended reading for self-study of theoretical content</p> <p>To understand the proposed theoretical contents, the following bibliographic resources are proposed for reading: (Three and a half hours)</p> <ul> <li>For a quick introduction to the concept of neural networks, activation functions, loss function, gradient descent, backpropagation, chain rule, hyperparameters, we recommend reading: (30 minutes)</li> <li>Brief Introduction to Artificial Neural Networks. Section 3.</li> <li>To develop the concepts of linear regression, loss function, gradient descent and linear regression understood as a basic neural network, the following reading is proposed: (30 minutes)</li> <li>Dive into Deep Learning. Section 3.1. Linear Regression.</li> <li>https://d2l.ai/chapter_linear-regression/linear-regression.html</li> <li>To learn the concept of training and generalization error, overfitting or Cross-Validation we recommend reading: (30 minutes)</li> <li>Dive into Deep Learning. Section 3.6. Generalization.</li> <li>https://d2l.ai/chapter_linear-regression/generalization.html</li> <li>To introduce the applications of neural networks in classification problems and the concepts of \"Softmax Regression for Classification\", \"Loss function for classification\" and \"Cross-Entropy\" we recommend reading: (60 minutes)</li> <li>Dive into Deep Learning. Section 4. Linear Neural Networks for Classification </li> <li>https://d2l.ai/chapter_linear-classification/index.html</li> <li>To introduce the fundamental of multilayer perceptron the following reading is proposed: (60 minutes)</li> <li>Dive into Deep Learning. Section 5. Multilayer Perceptrons</li> <li>https://d2l.ai/chapter_multilayer-perceptrons/index.html</li> <li>5.1-5.2: Multilayer Perceptrons. Incorporating Hidden Layers, activation Functions (ReLU, Sigmoid; Tanh).</li> <li>5.3: Forward Propagation, Backward Propagation, and Computational Graphs</li> <li>5.4: Numerical Stability and Initialization </li> <li>5.5-5.6: Generalization and dropout</li> </ul> <p>References</p> <ul> <li>Dive into Deep Learning. Interactive deep learning book with code, math, and discussions Implemented with PyTorch, NumPy/MXNet, JAX, and TensorFlow https://d2l.ai/ </li> <li>Brief Introduction to Artificial Neural Networks. Culture Sciences del\u2019 Ing\u00e9nieur.</li> </ul> <p>Moodle Test</p> <ul> <li>The moodle test must be taken in advance of the practice session on February 5. </li> <li>The test will be open between February 1st at 9:00 am and February 4th at 11:59 pm. </li> <li>The test has a maximum duration of 75 minutes from the start.</li> <li>The test consists of 24 triple choice questions. </li> <li>Each wrong answer subtracts 1/3 of the value of a correct answer.</li> <li> <p>The mark for the test will be considered as one of the marks for the theoretical part of the course. See the overall evaluation of the course in the general conditions.</p> </li> <li> <p>Before you start the moodle test make sure you know all these concepts.</p> </li> <li> <p>About Machine Learning Problems (Supervised; Unsupervised; Regression; Classification)</p> </li> <li>About Training/test datasets and training/generalization error.</li> <li>About Cross-Validation</li> <li>About Over-fitting/under-fitting</li> <li>About the mathematical model for a neuron of a perceptron </li> <li>About bias </li> <li>About activation functions </li> <li>About loss and cost functions expressions</li> <li>About gradient descent</li> <li>About learning rate</li> <li>About forward and back-propagation</li> <li>About derivatives for back-propagation</li> <li>About Hyperparameters </li> <li>About Dropout</li> <li>About parameter initialization</li> <li> <p>About early stopping</p> </li> <li> <p>To start the Moodle test go to \"UACloud\" -&gt; \"Moodle\" -&gt; \"Aprendizaje Profundo\" -&gt; Test - FROM SHALLOW TO DEEP Neural Networks</p> </li> </ul> <p>Workbooks</p> <p>For the practical part of the course, two notebooks have been prepared to exercise the theoretical concepts acquired. The objective of the notebooks is to allow students to improve their skills in the implementation of neural network models from scratch. Two notebooks are proposed.</p> <ul> <li>Notebook 1: (60 minutes) (30 minutes in class + 30 minutes at home)</li> <li>What is a Perceptron?</li> <li>The main difference between activation functions.</li> <li>The main difference between loss functions</li> <li>Gradient Descent and Learning Rate</li> <li>We will work with a single-layer network with d input nodes and a single output node</li> <li> <p>Shallow Networks / Multilayer Perceptron (MLP)</p> </li> <li> <p>Notebook 2: (90 minutes) (60 minutes in class + 30 minutes at home)</p> </li> <li>In this practice we will work with deep neural networks, i.e. with more than two hidden layers. </li> <li>In addition, we will continue to introduce and put into practice fundamental concepts of deep learning.</li> <li>Overfitting / Underfitting</li> <li>Parameter Initialization</li> <li>Early Stopping</li> <li> <p>Dropout</p> </li> <li> <p>To work with the practice you will need to make a local copy of the notebook and answer all the questions that are posed. </p> </li> <li>For the delivery you will have to send a copy of the notebook with the answers. You will have to use the delivery control of \"UACloud -&gt; Evaluaci\u00f3n\" called \"FROM SHALLOW TO DEEP NN. NoteBook X\".</li> <li>The deadline for delivery of notebooks 1 and 2 will be on Wednesday, February 12.</li> </ul>"},{"location":"drl/","title":"Deep Reinforcement Learning","text":"<p>Read this page and follow the instructions before the lecture.</p>"},{"location":"drl/#introduction","title":"Introduction","text":"<p>Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards, which it uses to improve its decision-making over time.</p> <p>Deep Reinforcement Learning (DRL) is an extension of RL that incorporates deep learning techniques. Instead of relying on handcrafted features or representations of the environment, DRL algorithms use deep neural networks to directly learn from raw sensory input, enabling them to tackle more complex tasks and achieve superior performance.</p> <p>DRL adds more advantages by enabling high-dimensional or infinite state and action spaces through the use of universal function approximators such as neural networks. This allows DRL algorithms to handle a wide range of tasks that were previously infeasible with traditional RL methods.</p>"},{"location":"drl/#fundamentals-of-reinforcement-learning","title":"Fundamentals of Reinforcement Learning","text":"<p>RL is a paradigm in machine learning where an agent learns to make decisions by interacting with an environment. Unlike supervised learning, where the model learns from labeled data, or unsupervised learning, where the model identifies patterns in unlabeled data, RL takes a very different approach. Instead of learning from explicit examples, here the agent learns through a process of trial and error, receiving feedback in the form of rewards based on its actions.</p>"},{"location":"drl/#understanding-the-players","title":"Understanding the Players","text":"<p>In this setup, we have two main players: the agent and the environment. The agent is the learner, the one making decisions, while the environment is everything the agent interacts with. Together, they create a dynamic system where actions lead to consequences.</p>"},{"location":"drl/#the-decision-making-process","title":"The Decision-Making Process","text":"<p>At each step of the process, the agent observes the current state of the environment, selects an action based on its strategy or policy, executes that action, and receives a reward or penalty from the environment. This feedback loop is fundamental to the learning process.</p>"},{"location":"drl/#key-concepts","title":"Key Concepts","text":""},{"location":"drl/#states-actions-and-rewards","title":"States, Actions, and Rewards","text":"<ul> <li>State (S): The representation of the environment at a given moment.</li> <li>Action (A): The decision made by the agent based on the current state.</li> <li>Reward (R): The feedback received from the environment after taking an action.</li> </ul>"},{"location":"drl/#policies-and-value-functions","title":"Policies and Value Functions","text":"<ul> <li>Policy (\u03c0): The strategy or rule the agent uses to select actions in different states.</li> <li>Value Function (V): The expected cumulative reward an agent can achieve from a particular state.</li> <li>Q-Value Function (Q): The expected cumulative reward an agent can achieve by taking a particular action in a given state.</li> </ul>"},{"location":"drl/#paradigms-in-deep-reinforcement-learning","title":"Paradigms in Deep Reinforcement Learning","text":"<p>The ultimate goal of reinforcement learning is to find an optimal strategy or policy that maximizes the cumulative rewards over time. To attain this goal, there are different types of methods.</p>"},{"location":"drl/#value-based-methods","title":"Value-Based Methods","text":"<p>Value-based methods aim to learn the optimal value function, which estimates the expected gain of taking an action in a given state and following a specific policy thereafter. Deep Q-Networks (DQN) is a prominent example of a value-based method, where a deep neural network is trained to approximate the Q-function.</p>"},{"location":"drl/#policy-based-methods","title":"Policy-Based Methods","text":"<p>Policy-based methods directly learn the policy, i.e., the mapping from states to actions, without explicitly estimating the value function. This approach can be more effective in high-dimensional or continuous action spaces. Examples include the REINFORCE algorithm and its variants, which optimize the neural network parameters to maximize expected rewards from a given state.</p>"},{"location":"drl/#actor-critic-methods","title":"Actor-Critic Methods","text":"<p>Actor-Critic methods combine aspects of both value-based and policy-based approaches. They maintain two neural networks: one (the actor) learns the policy, while the other (the critic) learns the value function. The critic provides feedback to the actor by evaluating the chosen actions, helping to guide the policy towards better decisions.</p>"},{"location":"drl/#self-study","title":"Self-study","text":"<p>Now is the time to delve into certain aspects of DRL before the in-person class. You must ensure a clear understanding of:</p> <ol> <li>The RL paradigm and its fundamental elements (state, action, policy, reward, value...).</li> <li>DQN and REINFORCE algorithms, and their differences.</li> <li>The concept behind actor-critic approaches.</li> </ol> <p> The recommended lecture is the book Understanding Deep Learning by Simon J.D. Prince, specifically the parts concerning DRL:</p> <ul> <li>Introduction at Section 1.3 (Book pp. 11-12; PDF pp. 25-26)</li> <li>Chapter 19: Reinforcement Learning (Book pp. 373-412; PDF pp. 387-398)<ul> <li>You can skip sections 19.3.1, 19.5.1, 19.7</li> </ul> </li> </ul> <p>You are encouraged to consult any other materials. These might include, but are not limited to:</p> <ul> <li> <p>Online resources:</p> <ul> <li>RL/DRL: OpenAI Spinning Up: Part 1: Key Concepts in RL</li> </ul> </li> <li> <p>Courses:</p> <ul> <li>RL Course by David Silver (Google DeepMind) on Youtube</li> <li>CS 285: Deep Reinforcement Learning (Berkeley)</li> </ul> </li> <li> <p>Books:</p> <ul> <li>\"Reinforcement Learning: An Introduction\" by Richard S. Sutton and Andrew G. Barto</li> </ul> </li> </ul>"},{"location":"drl/#materials-for-coding-practice","title":"Materials for coding practice","text":"<p>During the in-person practice session we will solve a classic control problem called CartPole using DRL. To simulate the scenario, we will work with the framework Gymnasium. Below you will find a Google Colaboratory notebook that introduces this working environment. You must read and understand this notebook before the session.</p> <ul> <li>Introduction to Gymnasium framework [link to colab]</li> </ul>"},{"location":"fsl/","title":"Few and Zero Shot Learning (12th February 2024). Contents to prepare before (online)","text":"<p>Part of the following contents are copied and adapted to the AI master from the ISMIR 2022 tutorial created by Yu Wang, Hugo Flores Garc\u00eda, and Jeong Choi. This is shared under Creative Commons BY-NC-SA 4.0.</p>"},{"location":"fsl/#what-is-few-shot-learning-fsl-and-zero-shot-learning-zsl","title":"What is Few-Shot Learning (FSL) and Zero-Shot Learning (ZSL)?","text":"<p>Before we dive right into FSL and ZSL, we would like to start with a brief discussion about the labeled data scarcity problem to illustrate the motivation and relevance of few-shot and zero-shot learning.  </p> <p>Deep learning has been highly successful in data-intensive applications, but is often hampered when the dataset is small. A deep model that generalizes well typically needs to be trained on a large amount of labeled data. However, in some applications such as music, most datasets are small in size compared to datasets in other domains, such as image and text. This is not only because collecting musical data may be riddled with copyright issues, but annotating musical data can also be very costly. The annotation process often requires expert knowledge and takes a long time as we need to listen to audio recordings multiple times. Therefore, many current music studies are built upon relatively small datasets with less-than-ideal model generalizability. </p> <p>Researchers have been studying strategies to tackle this scarcity issue for labeled data. These strategies can be roughly summarized into two categories:</p> <ul> <li>Data: crowdsourcing, data augmentation, data synthesis, etc.</li> <li>Learning Paradigm: transfer learning, semi-supervised learning , etc.</li> </ul> <p>There are different challenges for each of these approaches. For example, crowdsourcing still requires a large amount of human effort with potential label noise, the diversity gain from data augmentation is limited, and models trained on synthetic data might have issues generalizing to real-world data.</p> <p>Even with the help of transfer learning or unsupervised learning, we often still need a significant amount of labeled data (e.g. hundreds of thousands of examples) for the target downstream tasks, which could still be hard for rare classes. </p> <p>Few-shot learning (FSL) and zero-shot learning (ZSL), on the other hand, tackle the labeled data scarcity issue from a different angle. They are learning paradigms that aim to learn a model that can learn a new concept (e.g. recognize a new class) quickly, based on just a handful of labeled examples (few-shot) or some side information or metadata (zero-shot). </p>"},{"location":"fsl/#an-example","title":"An example","text":"<p>To have a better idea of how FSL and ZSL differ from standard supervised learning, let's consider an example of training a musical instrument classifier. We assume that there is an existing dataset in which we have abundant labeled examples for common instruments.</p> <ul> <li>In a standard supervised learning scenario, we train the classification model on the training set <code>(guitar, piano)</code>, then at test time, the model classifies \"new examples\" of \"seen classes\" <code>(guitar, piano)</code>.</li> <li>In a few-shot learning scenario, we also train the model with the same training set that is available <code>(guitar, piano)</code>. But at test time, the goal is for the few-shot model to recognize new examples from unseen classes (like <code>(banjo, kazoo)</code>) by providing a very small amount of labeled examples.  </li> <li>In a zero-shot learning scenario, we train the model on the available training set <code>(guitar, piano)</code>. But at test time, the goal is for the zero-shot model to recognize new examples from unseen classes (like <code>(banjo, kazoo)</code>) by providing some side information or metadata (e.g. the instrument family, e.g. string, wind, percussion, etc.) or a text embedding.</li> </ul> <p></p>"},{"location":"fsl/#few-shot-learning-foundations","title":"Few-Shot Learning Foundations","text":"<p>It should come with no surprise that data is at the core of few-shot learning problems. This chapter covers the foundations of few-shot learning \u2013 namely how we think about and structure our data \u2013 when trying to learn novel, unseen classes with very little labeled data.</p> <p>When solving traditional classification problems, we typically consider a closed set of classes. That is, we expect to see the same set of classes during inference as we did during training. Few-shot learning breaks that assumption and instead expects that the classification model will encounter novel classes during inference. There is one caveat: there are a few labeled examples for each novel class at inference time. </p> <p></p> <p>In few-shot learning, we expect to see novel classes at inference time. We also expect to see a few labeled examples (a.k.a. \"shots\") for each of the novel classes. </p> <p>Transfer learning and data augmentation are often considered approaches to few-shot learning<sup>1</sup>, since both of these approaches are used to learn new tasks with limited data. However, we believe these approaches are extensive and deserve their own treatment, and so we will not cover them here. Instead, we will focus on the topic of meta-learning \u2013 or learning to learn \u2013 which is at the heart of recent advances for few-shot learning. Transfer learning and data augmentation are orthogonal to meta-learning and can be used in conjunction with meta-learning approaches.</p>"},{"location":"fsl/#defining-the-problem","title":"Defining the Problem","text":"<p>Consider that we would like to classify between  classes, and we have exactly  labeled examples for each of those classes. We say that few-shot models are trained to solve a -way, -Shot classification task. </p> <p></p> <p>A few-shot learning problem splits data into two separate sets: the support set (the few labeled examples of novel data) and the query set (the data we want to label).</p> <p>Few shot learning tasks divide the few labeled data we have and the many unlabeled data we would like to to label into two separate subsets: the support set and the query set. </p> <p>The small set of labeled examples we are given at inference time is the support set. The support set is small in size and contains a few () examples for each of the classes we would like to consider. The purpose of the support set is to provide some form of guidance to help the model learn and adapt to the novel classification task. </p> <p>Formally, we define the support set as a set of labeled training pairs , where:</p> <ul> <li>  is a -dimensional input vector.</li> <li>  is the class label that corresponds to .</li> <li>  refers to the set of examples with label .</li> <li>  is the size of the support set, where .  </li> </ul> <p>On the other hand, the query set \u2013 typically denoted as  \u2013 contains all of the examples we would like to label. We can compare the model's predictions on the query set to the true labels (i.e., ground truth) to compute the loss used for training the model. In evaluation, we can use these predictions to compute metrics such as accuracy, precision, and recall.</p>"},{"location":"fsl/#the-goal","title":"The Goal","text":"<p>The goal of few-shot learning algorithms is to learn a classification model  that is able to generalize to a set of  previously unseen classes at inference time, with a small support set of  examples for each previously unseen class.</p>"},{"location":"fsl/#meta-learning-learning-to-learn","title":"Meta Learning - Learning to Learn","text":"<p>In order for a classifier to be able to learn a novel class with only a few labeled examples, we can employ a technique known as meta learning, or learning to learn.</p> <p>Even though our goal in few shot learning is to be able to learn novel classes with very few labeled examples, we still require a sizable training dataset with thousands of examples. The idea is that we can learn how to learn new classes from this large training set, and then apply that knowledge to learn novel classes with very few labeled examples.</p>"},{"location":"fsl/#class-conditional-splits","title":"Class-conditional splits","text":"<p>In supervised learning, one typically creates a train/test split in the dataset while ensuring that the classes seen during training are the same as those seen during testing.</p> <p>In few-shot learning, because we'd like our model to generalize to novel classes at inference time, we must make sure that there is no overlap between classes in our train, and test sets.</p> <p>A train/test split with no overlap between classes is called a class-conditional split. </p>"},{"location":"fsl/#episodic-training","title":"Episodic Training","text":"<p>To take full advantage of a large training set for few-shot learning, we use a technique referred to as episodic training<sup>2</sup><sup>3</sup>. </p> <p></p> <p>Episodic training is an efficient way of leveraging a large training dataset to train a few-shot learning model.</p> <p>Episodic training aims to split each training iteration into it's own self-contained learning task. An episode is like a simulation of a few-shot learning scenario, typically with  classes and  labeled examples for each class -- similar to what we expect the model to be able to infer at inference time. </p> <p>During episodic training, our model will see a completely new -shot, -way classification task at each step. To build a single episode, we must sample a completely new support set and query set during each training step. </p> <p>Practically, this means that for each episode, we have to choose a subset of  classes from our training dataset and then sample  labeled examples (for the support set) and  examples (for the query set) for each class that we randomly sampled. </p>"},{"location":"fsl/#few-shot-learning-approaches","title":"Few-Shot Learning approaches","text":"<p>Now that we have a grasp of the foundations of few-shot learning,  we'll take a look at some of the most common approaches to the solving few-shot problems. </p> <p>When training a model to solve a few-shot learning task, we typically sample episodes from a large training dataset. An episode is a simulation of a few-shot learning task, where we sample  classes and  labeled examples for each class. As we have seen, training a deep model by sampling few-shot learning episodes from a large training dataset is known as episodic training.</p> <p>Here are the few-shot learning approaches covered in this document:</p> <ol> <li> <p>Metric-based few-shot learning</p> </li> <li> <p>Optimization-based few-shot learning</p> </li> </ol>"},{"location":"fsl/#1-metric-based-few-shot-learning","title":"1- Metric-Based Few-Shot Learning","text":"<p>Metric-based approaches to few-shot learning are able to learn an embedding space where examples that belong to the same class are close together according to some metric, even if the examples belong to classes that were not seen during training. </p> <p></p> <p>At the center of metric-based few-shot learning approches is a similarity metric, which we will refer to as . We use this similarity metric to compare how similar examples in the query set are to examples in the support set. After knowing how similar a query example is to each example in the support set, we can infer to which class in the support set the query example belongs to. Note that this is conceptually the same as performing a nearest neighbor search. </p> <p>This similarity comparison is typically done in the embedding space of some neural net model, which we will refer to as . Thus, during episodic training, we train  to learn an embedding space where examples that belong to the same class are close together, and examples that belong to different classes are far apart. This embedding model is sometimes also referred to as a backbone model.</p> <p>There are many different metric-based approaches to few-shot learning, and they all differ in how they define the similarity metric , and how they use it to compare query examples to support examples as well as formulate a training objective.</p> <p>Among the most popular metric-based approaches are Prototypical Networks<sup>4</sup>, Matching Networks<sup>2</sup>, and Relation Networks<sup>5</sup>.</p>"},{"location":"fsl/#prototypical-networks","title":"Prototypical networks","text":"<p>The figure above illustrates a 5-shot, 3-way classification task between tambourine (red), maracas (green), and djembe (blue). In prototypical networks, each of the 5 support vectors are averaged to create a prototype for each class (). The query vector  is compared against each of the prototypes using squared euclidean distance. The query vector (shown as ) is assigned to the class of the prototype that it is most similar to. Here, the prototypes  are shown as black circles. </p> <p>Prototypical networks<sup>4</sup> work by creating a single embedding vector  for each class in the support set, called the prototype. The prototype for a class is the mean of the embeddings of all the examples in the support set for that class. </p> <p>Siamese Neural Networks is a kind of metric learning predecessor for prototypical networks. Siamese networks embed all support objects and the query object into a latent space and do a pairwise comparison between the query and all other support objects. The label of the closest support object is assigned to the query. Prototypical networks improve by 1) requiring comparisons between query and support centroids, not individual samples, during inference and 2) suffering from less sample noise by taking the mean of support embeddings.</p> <p>The prototype (denoted as ) for a class  is defined as: </p> <p> </p> <p>where  is the set of all examples in the support set that belong to class ,  is an example in , and  is the backbone model we are trying to learn. </p> <p>After creating a prototype for each class in the support set, we use the euclidean distance between the query example and each prototype to determine which class the query example belongs to. We can build a probability distribution over the classes by applying a softmax function to the negated distances between a given query example and each prototype:</p> <p> </p> <p>where  is a query example,  is the prototype for class , and  is the squared euclidean distance between two vectors.</p>"},{"location":"fsl/#2-optimization-based-few-shot-learning","title":"2- Optimization-Based Few-Shot Learning","text":"<p>Optimization-based approaches focus on learning model parameters  that can easily adapt to new tasks, and thus new classes. The canonical method for optimization-based few-shot learning is Model-Agnostic Meta Learning (MAML<sup>6</sup>), and it's successors<sup>7</sup><sup>8</sup>. </p> <p>The intuition behind MAML is that some representations are more easily transferrable to new tasks than others. </p> <p></p> <p>For example, assume we train a model with parameters  to classify between (<code>piano</code>, <code>guitar</code>, <code>saxophone</code> and <code>bagpipe</code>) samples.  Normally, we would expect that these parameters  would not be useful for classifying between instruments outside the training distribution, like <code>cello</code> and <code>flute</code>. The goal of MAML is to be able to learn parameters  that are useful not just for classifying between the instruments in the training set, but also are easy to adapt to new instrument classification tasks given a support set for each task, like <code>cello</code> vs <code>flute</code>, <code>violin</code> vs <code>trumpet</code>, etc.</p> <p>In other words, if we have some model parameters , we want  to be adapted to new tasks using only a few labeled examples (a single support set) in a few gradient steps. </p> <p>The MAML algorithm accomplishes this by training the model to adapt from a starting set of parameters  to a new set of parameters  that are useful for a particular episode . This is performed for all episodes in a batch, eventually learning a starting set of parameters  that can be successfully adapted to new tasks using only a few labeled examples.</p> <p>Note that MAML makes no assumption of the model architecture, thus the \"model-agnostic\" part of the method.</p>"},{"location":"fsl/#the-maml-algorithm","title":"The MAML algorithm","text":"<p>The MAML<sup>6</sup> algorithm. The starting model parameters are depicted as , while the task-specific, fine-tuned parameters for tasks 1, 2, and 3 are depicted as , , and , respectively. </p> <p>Suppose we are given a meta-training set composed of many few-shot episodes , where each episode contains a support set and a train set . We can follow the MAML algorithm to learn parameters  that can be adapted to new tasks using only a few examples and a few gradient steps. </p> <p>Overview of the MAML<sup>6</sup> training algorithm:</p> <ol> <li>Initialize model parameters  randomly, choose step sizes  and .  </li> <li>while not converged do<ol> <li>Sample a batch of episodes (tasks) from the training set  </li> <li>for each episode  in the batch do<ol> <li>Using the current parameters , compute the gradient of the loss  for episode .</li> <li>Compute a new set of parameters  by fine-tuning in the direction of the gradient w.r.t. the starting parameters :   </li> </ol> </li> <li>Using the fine-tuned parameters  for each episode, make a prediction and compute the loss .</li> <li>Update the starting parameters  by taking a gradient step in the direction of the loss we computed with the fine-tuned parameters :      </li> </ol> </li> </ol> <p>At inference time, we are given a few-shot learning task with support and query set . We can use the learned parameters  as a starting point, and follow a process similar to the one above to make a prediction for the query set :  </p> <ol> <li>Initialize model parameters  to the learned parameters from meta-training.</li> <li>Compute the gradient  of the loss  for the test episode .</li> <li>Similar to step 6 of the training algorithm above, compute a new set of parameters  by fine-tuning in the direction of the gradient w.r.t. the starting parameters . </li> <li>Make a prediction using the fine-tuned parameters : .</li> </ol>"},{"location":"fsl/#zero-shot-learning-foundations","title":"Zero-Shot Learning Foundations","text":"<p>Zero-shot learning (ZSL) is yet another approach for classifying the classes that are not observed during training. Main difference from few-shot learning is that it does not require any additional label data for novel class inputs. </p> <p></p> <p>Therefore, in zero-shot learning, there is no further training step for unseen classes. Instead, during the training phase, the model learns how to use the side information that can potentially cover the relationship between any of both seen and unseen classes. After training, it can handle the cases where inputs from unseen classes are to be classified.</p> <p></p> <p>ZSL was originally inspired by human\u2019s ability to infer novel objects or create new categories dynamically based on prior semantic knowledge, where general relationship between seen and unseen classes are learned.</p> <p>Let's look into a case of an audio-based musical instrument classification task. </p> <p>First, given training audio and their associated class labels (seen classes), we train a classifier that projects input vectors onto the audio embedding space. </p> <p></p> <p>However, there isn't a way to make prediction of unseen labels for unseen audio inputs yet.</p> <p></p> <p>As forementioned we use the side information that can inform the relationships between both seen and unseen labels. </p> <p>There are various sources of the side information, such as class-attribute vectors infered from an annotated dataset, or general word embedding vectors trained on a large corpus of documents. </p> <p></p> <p>The core of zero-shot learning paradigm is to learn the compatibility function between the embedding space of the inputs and the side information space of their labels. </p> <ul> <li>Compatibility function :  <ul> <li>  : input embedding function.</li> <li>  : mapping function. </li> <li>  : label embedding function.</li> </ul> </li> </ul> <p></p> <p>A typical approach is to train a mapping function between the two. By unveiling relationship between the side information space and our input feature space, it is possible to map vectors from one space to the other.</p> <p></p> <p>After training, arbitrary inputs of unseen labels can be predicted to the corresponding class. </p> <p></p> <p>Another option is to train a separate zero-shot embedding space where the embeddings from both spaces are projected (a metric-learning approach).</p> <ul> <li>E.g. Training mapping functions  and  with a pairwise loss function :  <ul> <li>where  </li> </ul> </li> </ul> <p></p> <p>In this case, the inputs and the classes are projected onto another zero-shot embedding space.</p> <p></p> <p>This space-aligning technique is one of the main branches of zero-shot learning framework. </p>"},{"location":"fsl/#popular-fewzero-shot-learning-models","title":"Popular Few/Zero Shot learning models","text":"<p>So far, we've gone through the broad concepts of the major few and zero-shot learning paradigms. However, there are also recent approaches using other architectures that take advantage of massive data. </p> <p>In this section we describe two zero/few-shot mainstream alternatives with widespread application in the industry: OpenAI CLIP and GPT-3.</p>"},{"location":"fsl/#contrastive-language-image-pre-training-clip","title":"Contrastive Language-Image Pre-Training (CLIP)","text":"<p>Introduced by OpenAI in 2021, CLIP<sup>9</sup> uses an encoder-decoder architecture for multimodal zero-shot learning. The figure below explains how CLIP works.</p> <p></p> <p>CLIP (Contrastive Language\u2013Image Pre-training) builds on a large body of work on zero-shot transfer, natural language supervision, and multimodal learning. It inputs text snippets into a text encoder (TE) and images into an image encoder (IE). It trains the encoders to predict the correct class by matching images with the appropriate text descriptions.</p> <p>The IE takes an image, the TE takes text, both of which return vector representations of the input. To match the dimensions of their results, a linear transformation layer is added to both IE and TE. For IE, the authors advise using ResNet<sup>10</sup> or VisionTransformer<sup>11</sup>. For TE, Continuous BOW (CBOW).</p> <p>For pre-training, 400 million pairs of the form (image, text) are used, which are fed to the input of IE and TE. Then a matrix is \u200b\u200bconsidered, the element  of which is the cosine similarity from the normalized vector representation of the -th image and the -th textual description. </p> <p>Thus, the correct pairs will end up on the main diagonal. Finally, by minimizing the cross-entropy along each vertical and horizontal axis of the resulting matrix, we maximize its values \u200b\u200bon the main diagonal.</p> <p>Once CLIP has been trained, you can use it to classify images from any set of classes \u2014 simply submit this set of classes, presented as descriptions, to TE, and the image to IE, and see which class represents the cosine similarity of the image with the highest value.</p>"},{"location":"fsl/#openai-gpt-3","title":"OpenAI GPT-3","text":"<p>In 2020, OpenAI announced GPT-3. However, it wasn\u2019t just another size upgrade. The paper, entitled Language Models are Few-Shot Learners<sup>12</sup>, describes a generative language model with 175 billion parameters, 10x more than any previous language model. They published its performance on NLP benchmarks in which GPT-3 showed the improved capability to handle tasks purely via text interaction.</p> <p>Those tasks include zero-shot, one-shot, and few-shot learning, where the model is given a task de\ufb01nition and/or a few examples and must perform the task without additional training. That is, no \ufb01ne-tuning is used. It is as though humans perform a new language task from only a few examples of simple instructions. The question posed in the paper is: can a pre-trained language model become a meta-learner?</p> <p>To answer this, they use in-context learning. Below is a demonstrative diagram of how in-context learning works, where the model develops a broad set of capabilities and learns to adapt them to perform tasks defined via natural language examples.</p> <p></p> <p>The outer loop refers to the unsupervised pre-training where the model acquires language skills. On the other hand, the inner loop occurs when we feed forward a sequence of examples to the model, which learns the context from the sequence to predict what comes next. It is like a human reading a sequence of examples and thinking about the next instance. As the model uses the language skills learned during the pre-training phase to learn the context given in the sequence, no neural network parameter updates are involved in this learning phase. They call it in-context learning.</p> <p>The diagram\u2019s first (left-most) example provides a context for performing arithmetic addition. Then, the second (middle) demonstrates how to correct spelling mistakes. The last (right-most) provides examples of English-to-French word translations. Given the respective context, the model must learn how to perform the intended task. They tried this approach with GPT-2, but the result wasn\u2019t good enough to be a practical method of solving language tasks.</p> <p>Since then, however, they saw a growing trend in the capacity of transformer language models in terms of the number of parameters, bringing improvements to text generation and other downstream NLP tasks. They hypothesized that in-context learning would show similarly substantial gains with scale.</p> <p>Therefore, OpenAI researchers trained a 175 billion parameter language model (GPT-3) and measured its in-context learning abilities.</p>"},{"location":"fsl/#few-shot-one-shot-and-zero-shot-learning-in-gpt-3","title":"Few-Shot, One-Shot, and Zero-Shot Learning in GPT-3","text":"<p>They evaluated GPT-3 on three conditions:</p> <ul> <li>Zero-Shot allows no demonstrations and gives only instruction in natural language.</li> <li>One-Shot allows only one demonstration.</li> <li>Few-Shot (or in-context) learning allows as many demonstrations (typically 10 to 100).</li> </ul> <p>The below diagram explains the three settings (on the left) of GPT-3 evaluations, compared with the traditional fine-tuning (on the right).</p> <p></p> <p>The following graph shows the model performance on the learning task where it needs to remove extraneous (unnecessary) symbols from a word. The model performance improves over the number of in-context examples (), with or without a prompt (natural language task description), where  is zero-shot,  is one-shot, and  is few-short learning. It makes sense that the model performs better with a larger  as it can learn from more examples. Moreover, a prompt would give more context, improving the model\u2019s accuracy, especially where  is smaller. In other words, no prompt means that the model must infer what is being asked (i.e., guess the prompt) from the examples.</p> <p></p> <p>As we can see, the largest model with 175 billion parameters has the steepest improvement, proving that the larger capacity of the model increases the model\u2019s ability to recognize patterns from the in-context examples. Therefore, the main conclusion is that LLM size Does Matter To In-Context Learning.</p> <p>It should be reiterated that the accuracy improvement does not require gradient updates or fine-tuning. The increasing number of demonstrations given as conditioning allows the model to learn more contexts to improve its prediction accuracy.</p> <ol> <li> <p>Yisheng Song, Ting-Yuan Wang, Subrota Kumar Mondal, and Jyoti Prakash Sahoo. A comprehensive survey of few-shot learning: evolution, applications, challenges, and opportunities. ArXiv, 2022.\u00a0\u21a9</p> </li> <li> <p>Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS'16, 3637\u20133645. Red Hook, NY, USA, 2016. Curran Associates Inc.\u00a0\u21a9\u21a9</p> </li> <li> <p>Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations. 2017. URL: https://openreview.net/forum?id=rJY0-Kcll.\u00a0\u21a9</p> </li> <li> <p>Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\u00a0\u21a9\u21a9</p> </li> <li> <p>Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales. Learning to compare: relation network for few-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). June 2018.\u00a0\u21a9</p> </li> <li> <p>Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, 1126\u20131135. PMLR, 06\u201311 Aug 2017. URL: https://proceedings.mlr.press/v70/finn17a.html.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: learning to learn quickly for few shot learning. CoRR, 2017. URL: http://arxiv.org/abs/1707.09835, arXiv:1707.09835.\u00a0\u21a9</p> </li> <li> <p>Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot learning. In CVPR, 403\u2013412. 2019.\u00a0\u21a9</p> </li> <li> <p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. CoRR, 2021. URL: https://arxiv.org/abs/2103.00020, arXiv:2103.00020.\u00a0\u21a9</p> </li> <li> <p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, 2015. URL: http://arxiv.org/abs/1512.03385, arXiv:1512.03385.\u00a0\u21a9</p> </li> <li> <p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: transformers for image recognition at scale. CoRR, 2020. URL: https://arxiv.org/abs/2010.11929, arXiv:2010.11929.\u00a0\u21a9</p> </li> <li> <p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, 2020. URL: https://arxiv.org/abs/2005.14165, arXiv:2005.14165.\u00a0\u21a9</p> </li> </ol>"},{"location":"fsl_assignment/","title":"Few and Zero-Shot Learning assignment","text":"<p>This assigment must be submitted by February 19th using Moodle</p>"},{"location":"fsl_assignment/#prototypical-networks","title":"Prototypical Networks","text":"<p>In this exercise we will experiment with prototypical networks<sup>1</sup>, training a model for -shot, -way classification. For this, we will work with the Omniglot dataset<sup>2</sup>, which includes 1623 handwritten characters from 50 different alphabets. You can see some samples of this dataset below:</p> <p></p> <p>As discussed in the theory contents, the basic idea of prototypical networks resembles nearest neighbors to class prototypes. They compute the prototype of each class using a set of support examples and then calculate the distance between the query example and each the prototypes. The query example is classified based on the label of the prototype it\u2019s closest to, as can be seen in the following figure:</p> <p></p> <p>In summary,  is a softmax function to the negated distances between a given query example and each prototype.</p> <p>If you think that you didn't fully understand prototypical networks after reading the provided materials, please see also this video, which clearly explains their foundations.</p>"},{"location":"fsl_assignment/#exercise-1","title":"Exercise 1","text":"<p>First, open this Colab notebook which contains some code to be completed with TODO marks. You need to save a copy of this notebook to your Google Drive in order to make edits, and then upload the final <code>.ipynb</code> to Moodle.</p> <ol> <li> <p>Review and understand the code in the class <code>OmniglotEpisodicDataset</code>. The sampled batch is partitioned into support, i.e. the per-task training data, and query, i.e. the per-task test datapoints. The support will be used to calculate the prototype of each class and query will be used to compute the distance to each prototype. </p> </li> <li> <p>Complete the code with TODO marks in the <code>train</code> function. For implementing  you can use directly the Pytorch function <code>CrossEntropyLoss</code>, since it is a combination of softmax and cross-entropy. Specifically, <code>CrossEntropyLoss(x, y) := H(one_hot(y), softmax(x))</code>, where <code>one_hot</code> is a function that takes an index  and expands it into a one-hot vector.</p> </li> <li> <p>Run the training cell. After 20 epochs, the average loss when running in Google colab should be similar to 0.017.</p> </li> <li> <p>Complete the code with TODO marks in the <code>evaluate</code> function. For inference, instead of the loss function you should use <code>argmax</code> to obtain the predicted labels.</p> </li> <li> <p>Run the evaluation code. In Colab, the accuracy should be close to 98.66%.</p> </li> <li> <p>Answer the questions at the end of the notebook and discuss the results.</p> </li> </ol>"},{"location":"fsl_assignment/#few-shot-learning-using-openai-gpt","title":"Few-shot learning using OpenAI GPT","text":"<p>We know many of you are GPT fans, so we are going to make an assignment using this tool. </p> <p>As we have seen in the theory contents, very Large Language Models (LLM) can perform few-shot learning with minimal steps. </p> <p>Let's test an example by promping the following input to ChatGPT-4:</p> <pre><code>Input: Subpar acting. \nSentiment: Negative\nInput: Beautiful film. \nSentiment: Positive\nInput: Amazing. \nSentiment:\n</code></pre> <p>Hint: In the GPT interface, if you want to insert a new line without sending the prompt, simultaneously press shift + enter</p> <p>Run this prompt and check the result. We have just created a sentiment analysis classifier without any line of code, although it may have  limitations in more complex scenarios. By using the ChatGPI API you can even integrate your sentiment classifier into a webpage or an app.</p> <p>This is a simple example, but making reliable prompts for accurate few-shot learning sometimes require additional work. For example, have a look at this paper<sup>3</sup>. You can see in Fig. 4 how the order and balance of the positive/negative examples can affect the results.</p> <p>In recent GPT versions, the behaviour is a bit different than in the paper. For example, prompts with N/A are not accepted.</p> <p>The goal of the following exercise is assesssing your understanding of how to effectively employ few-shot learning techniques on GPT.</p>"},{"location":"fsl_assignment/#exercise-2","title":"Exercise 2","text":"<p>In this second exercise, we are going to make a few-shot classifier to classify between Rock and hip-hop genres from a short part of song lyrics. To achieve this goal, the classifier must be trained on a few examples of lyrics and their corresponding labels. Then, given new lyrics, it should ideally predict the song genre.</p> <p>An example prompt could be:  <pre><code>Input: Bitterness and burden\nCurses rest on thee\nSolitaire and sorrow\nAll eternity\nSave the Earth and claim perfection\nDeem the mass and blame rejection.\nOutput: Rock\n\nInput: Tell me who you loyal to\nIs it money? Is it fame? Is it weed? Is it drink?\nIs it comin' down with the loud pipes and the rain?\nBig chillin', only for the power in your name.\nOutput: Hip-hop\n</code></pre></p> <p>You can find more examples in this link, where you can search lyrics by genre or artist.</p> <p>The goal is to effective prompts, and check if ordering of the samples and balance of the classes may affect the results. For our few-shot scenario, try with  labeled samples for each of the 2 classes.</p> <p>For this exercise, you should use ChatGPT on a browser and select the GPT-4 legacy model. </p> <p>When using GPT\u20114 through a prompt (for example, via the ChatGPT web interface), the model operates entirely based on its pre\u2011trained internal knowledge and does not perform live lookups or access external databases in real time.</p> <p>Once done, please submit a PDF file (via Moodle) with the experiments you made and the conclusions.</p>"},{"location":"fsl_assignment/#assessment-criteria","title":"Assessment criteria","text":"<ul> <li>Prompt Engineering: Students will be evaluated on their ability to engineer effective prompts that leverage the few-shot examples. This includes the clarity of the prompt, the relevance of the examples to the test case, and the prompt's ability to guide the model towards the desired output.</li> <li>Model Interaction: Students may need to iteratively refine their prompts based on the model's responses, demonstrating an understanding of how different prompt structures influence the outcome.</li> <li>Critical Analysis: In addition to generating outputs, students should critically analyze the model's performance, identifying any biases, errors, or limitations in the generated responses.</li> </ul> <ol> <li> <p>Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\u00a0\u21a9</p> </li> <li> <p>Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332\u20131338, 2015. URL: https://www.science.org/doi/abs/10.1126/science.aab3050, arXiv:https://www.science.org/doi/pdf/10.1126/science.aab3050, doi:10.1126/science.aab3050.\u00a0\u21a9</p> </li> <li> <p>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: improving few-shot performance of language models. CoRR, 2021. URL: https://arxiv.org/abs/2102.09690, arXiv:2102.09690.\u00a0\u21a9</p> </li> <li> <p>Gemini Team et al. Gemini: a family of highly capable multimodal models. 2023. arXiv:2312.11805.\u00a0\u21a9</p> </li> </ol>"},{"location":"ftkd/","title":"Fine-tuning and model compression","text":""},{"location":"ftkd/#fine-tuning","title":"Fine-tuning","text":"<p>Fine-tuning is the process of taking a pre-trained model and updating its parameters with new data. This is a common practice in deep learning, where vision or language models are trained on large datasets and then fine-tuned on smaller datasets to adapt them to specific tasks. As you may know, the pre-training stage usually follows a self-supervised learning approach (image patching or token masking, for example), where the model is trained to predict some information already present in the data. In this way, the model learns to represent the data in a neutral manner that is useful for a wide range of tasks. Some pre-trained models are additionally fine-tuned on a supervised task, such as text-to-image generation, machine translation or instruction following, to name a few, but even in those cases the models can be further fine-tuned on more specific downstream task, such as generating manga-style images, translating new languages or acting as a conversational chatbot, for example.</p> <p>Catastrophic forgetting is a phenomenon that occurs in neural networks, particularly evident during the fine-tuning process of deep learning models. It describes the tendency of neural networks to completely forget previously learned information upon learning new data. This is a significant challenge when adapting a pre-trained model to a new task (that is,   fine-tuning), as the model may lose its ability to perform well on the original tasks it was trained on. This is especially relevant when we want to keep the performance in the first task while improving the performance in the second task. To mitigate catastrophic forgetting, several techniques have been proposed. For example, a term may be added to the loss function that penalizes the probability distribution of the model's predictions differing from the original model's predictions; the Kulback-Leibler divergence is a common choice for this term. Another way to tackle catastrophic forgetting in multi-task learning scenarios is to mix the training data from the original and the new tasks, so that the model is trained on both tasks at the same time.</p>"},{"location":"ftkd/#parameter-efficient-fine-tuning","title":"Parameter-efficient fine-tuning","text":"<p>Another aspect to consider when fine-tuning a model is the choice of the parameters to re-learn. When fine-tuning pre-trained models such as BERT, it may suffice to re-learn only the predictor at the last layer while keeping the rest of the model frozen. In general, depending on the tasks, some layers may be frozen while others are trained. In other cases, it may be necessary to re-learn the entire model. No matter the case, a fine-tuning strategy known as low-rank adaptation (LoRA) significantly reduces the number of parameters to be fine-tuned, while maintaining the performance of the model. To illustrate how LoRA works, consider the situation in which the parameters of a matrix  of the pre-trained model needs to be fine-tuned. Instead of obtaining via gradient descent a new matrix of the increments  to be applied to , this matrix is approximated by two smaller matrices  and . The new parameters are then obtained as:</p> <p> </p> <p>where  is of dimension ,  is , and  is , with  and  . Note that  and  are of much smaller size than  and therefore fine-tuning is considerably faster. The hyperparameter  is usually set to a small power of 2 usually around 64 or less. The matrices  and  are initialized so that  is close to zero at the beginning of training.</p> <p></p> <p>You may now distribute the so called LoRA adapters represented by the matrices  and  separately and merge their product with the original weights of the model at inference time. The adapters may easily be thousands of times smaller than the original weights. When fine-tuning a transformer model with LoRA, only a subset of the matrices are adapted (usually, those computing the query, key and value vectors in the self-attention mechanism), while the rest of the model is kept frozen. Fine-tuning can be even more memory efficient if LoRA is combined with quantization (see next section), resulting in the technique known as QLoRA.</p> <p>LoRA and QLoRA are one of the many techniques grouped under the term parameter-efficient fine-tuning.</p>"},{"location":"ftkd/#model-compression","title":"Model compression","text":"<p>As a result of using large training datasets, deep neural models often require a high number of parameters to learn adequate representations for a given task. However, it has been shown that these models tend to learn redundant representations, and this observation has sparked interest in developing smaller models that, in some cases, can achieve similar or even better results than their larger counterparts. These efforts to obtain smaller models are collectively referred to under the umbrella of model compression techniques. Note that training a large model is unavoidable in many cases, but the proposal here is to compress the model after training it, so that it can be deployed in resource-constrained environments.</p> <p>Model compression techniques include the following ones: </p> <ul> <li>pruning, where certain weights are zeroed out based on their importance, allowing for an efficient storage of the remaining weights in sparse matrices; other approaches prune entire layers; for example, a paper published in 2019 removed 38 out of 48 encoder heads in a transformer used for machine translation without significantly affecting the performance showing that only a few heads do the heavy lifting;</li> <li>quantization, which reduces the precision of the numerical values representing the weights, from 32-bit or 16-bit floating point numbers for each parameter to 8-bit integers; in some cases, the parameters may be represented with even fewer bits arriving to 4-bit, 2-bit and even 1-bit representations; quantization involves advanced techniques to determine the optimal quantization levels for each parameter, but their description is out of the scope of this course; libraries such as bitsandbytes can be used to quantize models;</li> <li>knowledge distillation, which involves training a smaller \"student\" model to mimic the behavior of a larger \"teacher\" model, effectively transferring the knowledge without retaining the original model's size; next section analyzes this technique in more detail.</li> </ul>"},{"location":"ftkd/#knowledge-distillation","title":"Knowledge distillation","text":"<p>We will consider sequence-to-sequence models to present three approaches to knowledge distillation. In this case, the teacher model outputs a probability distribution over the target vocabulary, and the student model is trained to predict the same distribution. The next figure, taken from the article that introduced sequence-level knowledge distillation, depicts an overview of the various approaches to knowledge distillation. First, in word-level distillation (left), cross-entropy is minimized between student/teacher distributions (yellow) for each word in the actual target sequence (ECD) and between the student and the one-hot data distribution (black). In sequence-level distillation (center), the student is trained on the teacher's highest-scoring beam search output (ACF). For sequence-level interpolation (right), the student learns from the teacher's beam search output closest to the target sequence (ECE).</p> <p></p>"},{"location":"recurrent/","title":"Recurrent neural networks","text":"<p>Recurrent neural networks (RNNs) are a class of neural networks that are designed to work with sequences of data, such as time series, text or audio. Unlike transformers, which process the entire input sequence at once, RNNs process the input sequence one element at a time, while maintaining an internal state that encodes information about the elements processed so far. This internal state is updated at each step of the sequence, and used to inform the next layer (in the case of middle layers) or the output of the network in the case of the last layer. The first uses of RNNs can be traced back to the 1980s, but soon it was discovered that they were difficult to train due to the vanishing gradient problem, which made them unable to learn the long-term dependencies that happen in many sequences when the output at a given time step depends on the input at a much earlier time step. This problem was partially solved with the introduction of the long short-term memory (LSTM) units that were designed to replace conventional neurons in previous RNNs. However, the arrival of transformers in 2017 made RNNs less popular, although not completely obsolete. Recently, RNNs have experienced a resurgence of interest due to the development of new architectures and training techniques, such as the receptance weighted key value (RWKV) model, that have made them more efficient and easier to train.</p>"},{"location":"recurrent/#recurrent-neural-networks_1","title":"Recurrent neural networks","text":"<p>To learn more about RNNs, proceed to read chapter 9 of the book \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin following  this link. Note that the link points to an archived version of the book, as the book is not finished yet and chapter contents change frequently. Althoug RNNs can be applied to multiple types of sequences, the book focuses on their application to text. Nevertheless, you will not find it difficult to apply the concepts to other types of non-symbolic sequences such as numerical time series.</p> <p>Firstly, study  sections 9.1 to 9.3 (estimated time: \ud83d\udd51 2 hours). Mind some obsolete statements in the text, such as the claim at the end of section 9.3.3 that \"this simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering\" which is no longer true. Except for the introduction of the recurrence in the architecture, most of the ideas (matrix multiplications to move from one vector space to another, activation functions, softmax predictors at the output, cross-entropy loss, representation of inputs as word embeddings, etc.) will not be new to you if you are already familiar with the basics of neural networks for natural language processing.  </p>"},{"location":"recurrent/#long-short-term-memory","title":"Long short-term memory","text":"<p>Skip section 9.4 and jump next to  section 9.5, which introduces the LSTM units, and lastly to section   9.6 (estimated time: \ud83d\udd51 1 hour). Now, skim through the rest of the chapter for a couple of minutes only: you will see that encoder-decoder architectures are also viable with RNNs and that an attention mechanism (not exactly the same as the one used in transformers) can be used to determine which parts of the representations (states) learned by the encoder are more relevant to the decoder at each time step.</p>"},{"location":"recurrent/#key-value-memories-in-neural-networks","title":"Key-value memories in neural networks","text":"<p>RNNs' state is a form of memory that is updated at each time step. In principle, specially when using LSTM cells, the states should be able to indefinitely store information from the past. However, in practice, the information stored in the state is limited by constraints in the architecture and the training process. This has led to the development of architectures that incorporate more explicit forms of memory, for example by using key-value memories. In this context, the key-value memory is a data structure that stores a set of key-value pairs, both represented as vectors; another vector known as the query is used to retrieve the value associated with a given key by computing the similarity between the query and the keys. In our settings, usually the query is not used to only retrieve one single key, but a combination of a number of keys weighted by their similarity to the query.</p> <p>A lot of proposals exist on how to augment recurrent networks or transformers with more explicit memories in order to overcome some of the challenges they face when dealing with long-context reasoning, factual recall, and efficient storage of associative knowledge. The idea of incorporating memory into neural networks is not new and dates back to associative memories such as the Hopfield networks in the seventies and eighties, and, more recently, to the neural Turing machines and the differentiable neural computers (DNC) in the last decade. Here, we discuss a series of more recent approaches. </p>"},{"location":"recurrent/#learned-memory-during-training","title":"Learned memory during training","text":"<p>In this approach, memory contents are learned and fixed during training, and then used as-is during inference. The memory layer consists of learnable key-value pairs, which are stored as model parameters. These key-value pairs are optimized during training and remain unchanged during inference. Some of the feed-forward layers in the transformer blocks are replaced with memory layers, which retrieve relevant knowledge from the memory bank and incorporate it into the residual stream. It seems to be crucial to replace only some feed-forward layers with memory layers, as replacing all layers with memory-based mechanisms leads to suboptimal performance. Note that feed-forward networks already act as implicit associative memories by mapping inputs to outputs, but explicit memory layers perform a more direct selection of relevant knowledge from a structured key-value store with potentially millions of entries.</p> <p>A memory lookup operation follows the following steps. First, the input embedding is transformed into a query vector . Then, the top- keys are selected from the memory bank by efficiently computing the dot-product similarity between the query and the keys. After this, the softmax function is applied to the dot products to obtain the relevance of each memory slot. Finally, the output is computed as a weighted sum of the values associated with the top- keys.</p>"},{"location":"recurrent/#read-write-memory-during-inference","title":"Read-write memory during inference","text":"<p>Unlike the previous approach, this method does not pre-train the memory values. Instead, it learns parameters that control what to store and retrieve dynamically during inference. The memory contents are updated as the model processes input sequences. DNCs are a well-known example of this. A more recent approach is the LM2 model. Interestingly, the LM2 model works as an adaptation of the LSTM principles to memory-based models in the sense that it uses an input gate to control the amount of new information that is stored in the memory, a forget gate to control the amount of information that is kept from the previous memory state, and an output gate to control the amount of information that is read from the memory and used to update the output of the model.</p> <p>In the LM2 model, the memory module consists of a bank of vector slots, each of which stores a simple vector rather than explicit key-value pairs. For each slot in the memory bank, learnable projection matrices are used to generate both keys and values. Similarly, input token embeddings are projected into query vectors via another learned linear transformation. Using the standard attention mechanism, queries are matched against keys to compute attention scores, which then weight the corresponding memory values to produce the output of the memory module . Optionally, only the top- memory slots may be considered when computing the output of the memory module. Note that  integrates information from the input and the memory.</p> <p>The final output of the transformer block, , is computed by combining  with the output of the standard self-attention mechanism:</p> <p> </p> <p>where  is the memory-modulated contribution, scaled by a learned scalar gate , which is obtained through another trainable projection:</p> <p> </p> <p>At each step, the memory state is updated dynamically as follows:</p> <p> </p> <p>where:</p> <ul> <li>  and  are learned gating functions obtained via trainable projection matrices,</li> <li>and  is the memory state at step .</li> </ul> <p>This update rule determines how much of the retrieved memory content is incorporated into the new memory state and how much of the existing memory is retained.</p>"},{"location":"recurrent/#recurrent-memory-transformer","title":"Recurrent memory transformer","text":"<p>Additionally, recent developments in memory-augmented transformers have introduced new architectures that further enhance long-context processing. The recurrent memory transformer (RMT) introduces a segment-level recurrent memory mechanism, allowing the model to store and transfer information across long sequences without modifying the core transformer structure. It achieves this by adding special memory tokens that persist across segments, effectively extending the model's context length. Building upon this, the associative recurrent memory transformer (ARMT) enhances RMT by incorporating associative memory, enabling more efficient information storage and retrieval. This approach combines self-attention with memory updates, improving long-term reasoning and factual recall in extremely long-context tasks.</p>"},{"location":"recurrent/#titans-long-term-memory-in-neural-networks","title":"Titans: long-term memory in neural networks","text":"<p>Titans architecture, published in 2025, integrates a hierarchical memory system into a transformer framework, enabling efficient long-term storage and retrieval of past information. It introduces three interconnected memory components: short-term attention-based memory, which processes immediate dependencies; long-term neural memory, which retains historical context beyond the local window; and persistent memory, which stores task-specific knowledge. This layered memory design allows Titans to surpass conventional transformers in handling extensive sequences while maintaining fast inference and scalability.</p> <p>A core innovation in Titans is its memory decay mechanism, which functions similarly to a forget gate in recurrent models. This decay selectively removes outdated information while preserving relevant past data, dynamically adapting based on the sequence. Unlike fixed-size memory compression methods in standard transformers, Titans utilize adaptive decay functions that regulate memory retention over time. This ensures that memory is continuously updated while preventing overflow, addressing a key limitation of long-context processing in transformers.</p> <p>Titans employ a surprise-based learning approach to prioritize memory updates. When encountering new input, the model evaluates its divergence from expected patterns, using a gradient-based surprise metric to determine its relevance. Information that significantly deviates from learned patterns is reinforced in memory, while predictable or redundant data is gradually phased out. By incorporating this mechanism into transformer layers, Titans effectively balance memory utilization, allowing for both dynamic adaptation and efficient long-term recall within a scalable attention-based framework.</p>"},{"location":"recurrent/#other-recurrent-or-hybrid-architectures","title":"Other recurrent or hybrid architectures","text":"<p>As already mentioned, a renaissance (or a RNNaissance as some people called it when LSTM units were proposed in the late 1990s) of interest in RNNs has taken place recently motivated by the development of new architectures and training techniques that surpass some limitations of the transformer model. One of these limitations is the quadratic complexity of the self-attention mechanism, which makes it difficult to scale to very long sequences (context length) of thousands of tokens given the current memory capacity of GPUs. This quadratic complexity may be observed by considering that, given a sequence of length , the self-attention mechanism at each transformer head has to store  dot products. When used as generators of sequences at inference time, both architectures, RNN and transformers, have to process the sequence one token at a time, but at training time, the transformer can process the whole sequence at once in a parallel manner, while the RNN has to process it one token at a time to incrementally update its internal state. In addition to this, the softmax operation in the self-attention mechanism is also a bottleneck in terms of computational complexity; actually, different approaches have been proposed to mitigate (linearize) the impact of the softmax, thereby allowing for context lengths of up to one million tokens.</p> <p>All the aforementioned issues have motivated the search for the holy grail of a model that combines the best performance with parallelizable training and efficient inference, as represented by the following image taken from the retentive network (RetNet) paper:</p> <p></p> <p>As an example, the RWKV (for receptance weighted key value, pronounced as RaWKuV) architecture combines efficient parallelizable training with the efficient inference capabilities of RNNs. This architecture employs a linear attention mechanism, enabling the model to be formulated as either a transformer or an RNN. This dual formulation allows for parallelized computations during training while maintaining constant computational and memory complexity during inference. Models based on RWKV with billions of parameters have been trained, resulting in the largest RNNs to date. In preliminary experiments, the RWKV architecture has been shown to be competitive with similarly sized transformers. </p> <p>Read a brief description of the RWKV architecture in this  post by Johan Sokrates Wind (estimated time: \ud83d\udd51 30 minutes). We will not delve into the mathematical details of the RWKV architecture in this course, but see in the next figure a schematic representation of its underlying architecture which proves that it is not so different from the transformer architecture, at least at bird's eye view:</p> <p></p> <p>Optionally, if you are interested in the mathematical details, you can read the original paper.</p> <p>Recent times have also seen the development of other efficient architectures such as the already mentioned retentive networks or the Mamba model. The study of these architectures is out of the scope of this course and left as an exercise for the student. It is also interesting to note that there are some theoretical studies that try to determine to which degree both architectures can be considered equivalent; for example, it has been shown that transformers can be conceptualized as a special case of RNNs with unlimited hidden state size.</p>"},{"location":"recurrent/#additional-techniques-for-speeding-up-neural-networks","title":"Additional techniques for speeding up neural networks","text":"<p>In parallel to the development of new architectures to overcome the limitations of transformers, scaling transformers to longer sequences is one of the most active research areas. Once the attention mechanism is identified as the primary bottleneck, techniques like FlashAttention exploit specific GPU memory characteristics to achieve significant memory savings and runtime acceleration without resorting to approximations, thereby preserving the integrity of the attention's calculations. Two notable techniques, FlashAttention and its more advanced successor FlashAttention-2, further leverage GPU properties to significantly enhance processing speeds, potentially increasing the speed of the models by factors of 4 to 8 times compared to models without these optimizations. These mechanisms are now integrated into many deep learning libraries.</p>"},{"location":"recurrent/#time-series-prediction","title":"Time-series prediction","text":"<p>Traditionally, one of the most common applications of RNNs has been time-series prediction. In this context, RNNs are used to predict the next value of a time series given the previous values or to classify the time series into different categories. With the advent of transformers, the use of RNNs for time-series prediction has decreased, but they are still used in many cases, especially when transformer's complexity bottlenecks become a problem. In order to make the use of transformers practical for time-series prediction, some techniques have been developed to make the self-attention mechanism more efficient; they are complemented with the addition of more elaborated task-oriented positional embeddings (see, for example, the Informer model) that explicitly encode the time information (day, month, season, etc.) of the data. Nevertheless, traditional, non-neural and considerably simpler techniques such as ARIMA can never be discarded, at least as a baseline to compare the performance of more complex models with.</p>"},{"location":"ssl/","title":"Self-Supervised Learning","text":"<p>Some of the following contents are adapted from the paper entitled A Cookbook of Self-Supervised Learning<sup>1</sup>. </p> <p>Self-Supervised Learning (SSL), dubbed \u201cthe dark matter of intelligence\u201d, is a promising path to advance machine learning. As opposed to supervised learning, which is limited by the availability of labeled data, self-supervised approaches can learn from vast unlabeled data. </p> <p>SSL underpins deep learning\u2019s success in natural language processing leading to advances from automated machine translation to large language models trained on web-scale corpora of unlabeled text. SSL methods for computer vision have been able to match or in some cases surpass models trained on labeled data, even on highly competitive benchmarks like ImageNet. SSL has also been successfully applied across other modalities such as video, audio, and time series.</p> <p>Self-supervised learning defines a pretext task based on unlabeled inputs to produce descriptive and intelligible representations. No labelled data is required for SSL, although the models used are actually supervised.</p> <p>In natural language, a common SSL objective is to mask a word in the text and predict the surrounding words. This objective encourages the model to capture relationships among words in the text without the need for any labels. The same SSL model representations can be used across a range of downstream tasks such as translating text across languages, summarizing, or even generating text, along with many others. </p> <p>In computer vision, analogous pretext tasks exist with models learning to predict masked patches of an image or representation. Other SSL objectives encourage two views of the same image, formed by say adding color or cropping, to be mapped to similar representations.</p> <p>In self-supervised learning, one trains a model to solve a so-called pretext task on a dataset without the need for human annotation by creating positive (and sometimes, negative pairs) from unlabeled data.  The pretext tasks must be carefully designed to encourage the model to capture meaningful features and similarities in the data. </p> <p>The main objective, however, is to transfer this model to a target (downstream) task. The downstream task is the knowledge transfer process of the pretext model to a specific task. For this, one of the most effective transfer strategies is fine-tuning. After training the pretext task, the learned embeddings can easily be used for another task such as image classification or text summarization. </p> <p></p> <p>With the power to train on vast unlabeled data comes many benefits. While traditional supervised learning methods are trained on a specific task often known a priori based on the available labeled data, SSL learns generic representations useful across many tasks. SSL can be especially useful in domains such as medicine where labels are costly or the specific task can not be known a priori. There\u2019s also evidence SSL models can learn representations that are more robust to adversarial examples, label corruption, and input perturbations\u2014and are more fair\u2014compared to their supervised counterparts. Consequently, SSL is a field garnering growing interest. </p> <p>For this course, we are following the categorization in <sup>1</sup>, in which methods are divided into three families:</p> <ul> <li>Deep Metric Learning</li> <li>Self-Distillation</li> <li>Canonical Correlation Analysis</li> </ul> <p>A single model of each family is selected: SimCLR<sup>2</sup>, BYOL<sup>3</sup>, and VicReg<sup>4</sup>.</p> <p>To understand these recent methods, you should first know the basics of data augmentation and the origins of SSL.</p>"},{"location":"ssl/#data-augmentation","title":"Data Augmentation","text":"<p>SSL methods often begin with data augmentation, which involves applying various transformations or perturbations to unlabeled data to create diverse instances (also called augmented views).</p> <p>The goal of data augmentation is to increase the variability of the data and expose the model to different perspectives of the same instance. Common data augmentation techniques include cropping, flipping, rotation, random crop, and color transformations. By generating diverse instances, contrastive learning ensures that the model learns to capture relevant information regardless of variations in the input data.</p> <p>You can also chain the different techniques together:</p> <p></p> <p></p> <p>\ud83d\udca1 If you want to learn more about data augmentation, please read this link.</p>"},{"location":"ssl/#origins-of-ssl","title":"Origins of SSL","text":"<p>Contemporary SSL methods build upon the knowledge from early experiments, which form the foundation for many of the modern methods.</p> <p>Learning spatial context methods such as RotNet<sup>5</sup> train a model to understand the relative positions and orientations of objects within a scene. In particular, RotNet applies a random rotation and then asks the model to predict the rotation.</p> <p>The RotNet model is trained with random augmentations of images at 0, 90, 180 and 270 degrees. A standard  classification network is trained to predict the rotation. Then, the learned features can be used to initialize the weights of a network for another downstream task. </p> <p>Despite the simplicity of this self-supervised formulation, as can be seen in the experimental section of the paper, the features learned achieve dramatic improvements on the unsupervised feature learning benchmarks.</p>"},{"location":"ssl/#the-deep-metric-learning-family","title":"The Deep Metric Learning Family","text":"<p>The Deep Metric Learning (DML) family of methods is based on the principle of encouraging similarity between semantically transformed versions of an input. It includes methods such as SimCLR<sup>2</sup>, NNCLR, MeanSHIFT or SCL, among others. DML originated with the idea of contrastive loss, which transforms this principle into a learning objective. </p> <p>Contrastive learning leverages the assumption that similar instances should be closer together in a learned embedding space, while dissimilar instances should be farther apart. By framing learning as a discrimination task, contrastive learning allows models to capture relevant features and similarities in the data.</p> <p></p> <p>Contrastive Learning can be used in a Supervised or a Self-Supervised manner. </p> <p>In the Few-Shot Learning block, we already used Contrastive Learning methods in a supervised way, using labelled data for training models explicitly to differentiate between similar and dissimilar instances according to their class. Therefore, the loss in this case tries to put together embeddings from the same class. The objective is to learn a representation space where instances from the same class are clustered closer together, while instances from other classes are pushed apart.</p> <p>Self-supervised contrastive learning  takes a different approach by learning representations from unlabeled data without relying on explicit labels. Since no labels are available, to identify similar inputs, methods often form variants of a single input using known semantic preserving transformations via data augmentation.  The augmented samples are called views. The variants of the inputs are called positive pairs or examples; the samples we wish to make dissimilar are called negatives. So here, the loss tries to put together embeddings from the same view since we don't have information of the labels.</p>"},{"location":"ssl/#simclr","title":"SimCLR","text":"<p>SimCLR<sup>2</sup>, a Simple framework for Contrastive Learning of visual Representations, is a technique introduced by Prof. Hinton\u2019s Group.</p> <p></p> <p>SimCLR learns representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss in the latent space, as shown below.</p> <p></p> <p>SimCLR results were impressive, showing that  unsupervised learning benefits more from bigger models than its supervised counterpart.</p> <p>Homework</p> <ul> <li> Read this post to understand the basics of SimCLR. Estimated time: \ud83d\udd51 30 min.</li> </ul>"},{"location":"ssl/#the-self-distillation-family","title":"The Self-Distillation Family","text":"<p>Self-distillation methods such as BYOL<sup>6</sup>, SimSIAM, DINO, along with their variants rely on a simple mechanism: feeding two different views to two encoders, and mapping one to the other by means of a predictor. To prevent the encoders from collapsing by predicting a constant for any input, various techniques are employed. A common approach to prevent collapse is to update one of the two encoder weights with a running average of the other encoder\u2019s weights.</p>"},{"location":"ssl/#byol","title":"BYOL","text":"<p>BYOL (bootstrap your own latent)<sup>6</sup> introduced self-distillation as a means to avoid collapse. </p> <p></p> <p>BYOL uses two networks along with a predictor to map the outputs of one network to the other. The network predicting the output is called the online or student network while the network producing the target is called the target or teacher network. Each network receives a different view of the same image formed by image transformations including random resizing, cropping, color jittering, and brightness alterations. </p> <p>The online (student) network is updated throughout training using gradient descent. The target (teacher) network is updated with an exponential moving average (EMA) updates of the weights of the online network. The slow updates induced by exponential moving average creates an asymmetry that is crucial to BYOL\u2019s success.</p> <p>BYOL achieves higher performance than state-of-the-art contrastive methods without using negative pairs at all. Instead, it uses two networks that learn from each other to iteratively bootstrap the representations, by forcing one network to use an augmented view of an image to predict the output of the other network for a different augmented view of the same image. </p> <p>BYOL almost matches the best supervised baseline on top-1 accuracy on ImageNet and beats out the self-supervised baselines.</p> <p>Homework</p> <ul> <li> Read this post to understand the basics of BYOL. Estimated time: \ud83d\udd51 45 min.</li> </ul>"},{"location":"ssl/#the-canonical-correlation-analysis-family","title":"The Canonical Correlation Analysis Family","text":"<p>The SSL canonical correlation analysis family originates with the Canonical Correlation Framework<sup>7</sup>. The high-level goal of CCA is to infer the relationship between two variables by analyzing their cross-covariance matrices. It includes methods such as VicReg<sup>4</sup>, BarlowTwins, SWAV or W-MSE, among others. </p>"},{"location":"ssl/#vicreg","title":"VicReg","text":"<p>VICReg<sup>4</sup> has the same basic architecture as its predecessors; augmented positive pairs are fed into Siamese encoders that produce representations, which are then passed into Siamese projectors that return projections.</p> <p>However, unlike previous models, VicReg requires none of the following: negative examples, momentum encoders, asymmetric mechanisms in the architecture, stop-gradients, predictors, or even normalization of the projector outputs. Instead, the heavy lifting is done by VICReg\u2019s objective function, which contains three main terms: a variance term, an invariance term, and a covariance term.</p> <p>VICReg balances three objectives based on co-variance matrices of representations from two views: variance, invariance, co- variance. Regularizing the variance along each dimension of the representation prevents collapse, the invariance ensures two views are encoded similarly, and the co-variance encourages different dimensions of the representation to capture different features.</p> <p>Homework</p> <ul> <li> Read this post to understand the basics of VicReg. Estimated time: \ud83d\udd51 30 min.</li> </ul> <p>Using this code you can train SimCLR, BYOL or VicReg on ImageNet, STL-10, and CIFAR-10 and compare their results.</p> <ol> <li> <p>Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, Avi Schwarzschild, Andrew Gordon Wilson, Jonas Geiping, Quentin Garrido, Pierre Fernandez, Amir Bar, Hamed Pirsiavash, Yann LeCun, and Micah Goldblum. A cookbook of self-supervised learning. 2023. arXiv:2304.12210.\u00a0\u21a9\u21a9</p> </li> <li> <p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. CoRR, 2020. URL: https://arxiv.org/abs/2002.05709, arXiv:2002.05709.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - a new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, 21271\u201321284. Curran Associates, Inc., 2020. URL: https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf.\u00a0\u21a9</p> </li> <li> <p>Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: variance-invariance-covariance regularization for self-supervised learning. 2022. arXiv:2105.04906.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. CoRR, 2018. URL: http://arxiv.org/abs/1803.07728, arXiv:1803.07728.\u00a0\u21a9</p> </li> <li> <p>Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - a new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, 21271\u201321284. Curran Associates, Inc., 2020. URL: https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf.\u00a0\u21a9\u21a9</p> </li> <li> <p>Harold Hotelling. Relations Between Two Sets of Variates, pages 162\u2013190. Springer New York, New York, NY, 1992. URL: https://doi.org/10.1007/978-1-4612-4380-9_14, doi:10.1007/978-1-4612-4380-9_14.\u00a0\u21a9</p> </li> </ol>"}]}